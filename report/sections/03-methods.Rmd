* Ordinary Least Squares:

This regression method invloves determining a linear relationship between the independent variables and the dependent variable (Balance).

The model is as follows:
$$Y = \B_0 + \B_1X_1 + ... + \B_nX_n + \u$$

Using the data, the function **lm()** will hold all independnet varaibles constant except one and see the effects that has on the dependent variable. Repeating this process for each variable, it will return values for $$(\B_0, ..., \B_n)$$ thus allowing us to estimate the realationship between each variable and the depenedent variable. For each independent variable $$\X_i$$ it's predicted effect on the dependent variable is $$\B_i$$.

*Ridge:

The ridge regression is a regression that uses a shrinkage method to model the effect of the independent variables on the dependent variable. It uses a tuning parameter to calcualte what the shrinkage of each variable should be. Using the function **cv.glmnet()** and the tuning parameter ($\lambda\$), we can estimate the coefficients for each of the independent variables using the value for the tuning parameter that minimizes $\sum_{i=1}^n (y_i-\sum_{j=1}^p x_{ij}\beta_j)^2$.

* Lasso:

The lasso regression is also a shrinkage regression method. The main idea is the same as the ridge regression but lasso uses a different tuning parameter. 


* Principle Components Regression:

If we get different values for coefficient in summary from our regression and our correlation coefficient then something is wrong with our model. It suggests there is multicolineraity of variables. Thus we impliment the Principle Components Regression, which weights original variables and combines them into another component.

$$\X_1 --> \w_1                                     ---> \B_1$$
$$\X_2 --> \w_2 --> principal component (1, 2, ... k) ---> \B_2 ---> yhat$$
$$\X_3 --> \w_3                                     ---> \B_k

The function **pcr()** finds a matrix containing principle components and uses those principle components as new predictors to fit y. This gets rid of multicolineraity issues because the principal components are made with eigenvectors and form framework with corresponding space that they generate, so they are already orthogonal and we can calculate the inverse of the matrix without problems.


* Partial Least Squares Regression:

The Partial Least Squares Regression is similar to the PCR regression except it uses the dependent variable in the determination of the weights for each of the independent variables. 

Using Mean Squared Error:

To find the effectiveness of each regression (and ultimately to find which regression model best fits our data), we computed the mean squared error using this formula:

$$Mean Squared Error = \frac{1}{n}\sum_{i=1}^{n}(y_i - y_{hati})^2$$
